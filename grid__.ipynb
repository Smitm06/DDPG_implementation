{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873dc9-74b2-49a8-bf57-ab3afd989b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the grid size\n",
    "grid_size = (5, 5)  # A 5x5 grid\n",
    "\n",
    "\n",
    "grid = np.zeros(grid_size)\n",
    "\n",
    "\n",
    "obstacles = [(1, 1), (2, 2), (3, 3)]  # List of obstacle positions\n",
    "\n",
    "\n",
    "for obstacle in obstacles:\n",
    "    grid[obstacle] = -1  \n",
    "\n",
    "\n",
    "start = (0, 0)  # Starting point A\n",
    "end = (4, 4)  # Destination point B\n",
    "\n",
    "\n",
    "grid[start] = 1  \n",
    "grid[end] = 2  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self, grid_size, start, end, obstacles):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.obstacles = obstacles\n",
    "        self.state = start\n",
    "\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions: up, down, left, right\n",
    "        self.observation_space = spaces.Box(low=0, high=max(grid_size), shape=(2,), dtype=int)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return np.array(self.state, dtype=int)\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0 and y < self.grid_size[1] - 1:  # Up\n",
    "            y += 1\n",
    "        elif action == 1 and y > 0:  # Down\n",
    "            y -= 1\n",
    "        elif action == 2 and x > 0:  # Left\n",
    "            x -= 1\n",
    "        elif action == 3 and x < self.grid_size[0] - 1:  # Right\n",
    "            x += 1\n",
    "\n",
    "        next_state = (x, y)\n",
    "\n",
    "        # Calculate reward\n",
    "        current_distance = abs(x - self.end[0]) + abs(y - self.end[1])\n",
    "        new_distance = abs(x - self.end[0]) + abs(y - self.end[1])  \n",
    "\n",
    "        # Calculate reward\n",
    "        if next_state in self.obstacles:\n",
    "            reward = -1  \n",
    "            done = True\n",
    "        elif next_state == self.end:\n",
    "            reward = 10 \n",
    "            done = True\n",
    "        else:\n",
    "            if new_distance > current_distance:\n",
    "                reward = -1 \n",
    "            else:\n",
    "                reward = -0.1  \n",
    "            done = False\n",
    "    \n",
    "        self.state = next_state\n",
    "        return np.array(self.state, dtype=int), reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle] = -1\n",
    "        grid[self.start] = 1\n",
    "        grid[self.end] = 2\n",
    "        grid[self.state] = 3\n",
    "        print(grid)\n",
    "\n",
    "# Define Actor and Critic networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.layer1(state))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.softmax(self.layer3(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "        self.batch_size = 64\n",
    "        self.discount = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.epsilon = 0.2  # Exploration rate\n",
    "        \n",
    "    def select_action(self, state, explore=True):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        if explore and np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(0, 4)  # Random action\n",
    "        else:\n",
    "            action_probs = self.actor(state).cpu().data.numpy().flatten()\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        state, next_state, action, reward, done = zip(*batch)\n",
    "        \n",
    "        state = torch.FloatTensor(np.array(state)).to(device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state)).to(device)\n",
    "        action = torch.LongTensor(action).to(device).unsqueeze(1)\n",
    "        reward = torch.FloatTensor(reward).to(device).unsqueeze(1)\n",
    "        done = torch.FloatTensor(done).to(device).unsqueeze(1)\n",
    "        \n",
    "        # One-hot encode actions\n",
    "        action_one_hot = torch.zeros(self.batch_size, self.actor.layer3.out_features).to(device)\n",
    "        action_one_hot.scatter_(1, action, 1)\n",
    "        \n",
    "        # Compute target Q value\n",
    "        with torch.no_grad():\n",
    "            next_action_probs = self.actor_target(next_state)\n",
    "            next_action = next_action_probs.argmax(dim=1, keepdim=True)\n",
    "            next_action_one_hot = torch.zeros(self.batch_size, self.actor.layer3.out_features).to(device)\n",
    "            next_action_one_hot.scatter_(1, next_action, 1)\n",
    "            target_q = self.critic_target(next_state, next_action_one_hot)\n",
    "            target_q = reward + ((1 - done) * self.discount * target_q)\n",
    "        \n",
    "        # Get current Q estimate\n",
    "        current_q = self.critic(state, action_one_hot.float())\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        action_probs = self.actor(state)\n",
    "        action = action_probs.argmax(dim=1, keepdim=True)\n",
    "        action_one_hot = torch.zeros(self.batch_size, self.actor.layer3.out_features).to(device)\n",
    "        action_one_hot.scatter_(1, action, 1)\n",
    "        actor_loss = -self.critic(state, action_one_hot.float()).mean()\n",
    "        \n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def add_to_replay_buffer(self, state, next_state, action, reward, done):\n",
    "        self.replay_buffer.append((state, next_state, action, reward, done))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = GridWorldEnv(grid_size, start, end, obstacles)\n",
    "agent = DDPGAgent(state_dim=2, action_dim=4, max_action=3)\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset environment for each new episode\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    print(f\"Starting Episode {episode + 1}\")\n",
    "    \n",
    "    while not done:\n",
    "        # Select and clip action\n",
    "        action = agent.select_action(state)\n",
    "        action = int(np.clip(action, 0, 3))  # Ensure action is within valid range and convert to int\n",
    "        \n",
    "        # Execute the action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Store transition in replay buffer\n",
    "        agent.add_to_replay_buffer(state, next_state, action, reward, done)\n",
    "        \n",
    "        # Train the agent\n",
    "        agent.train()\n",
    "        \n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Print the grid at each step\n",
    "        env.render()\n",
    "    \n",
    "    # Print final episode information\n",
    "    print(f\"Episode {episode + 1} completed. Total Reward: {episode_reward}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
